{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOYjLec3isMw57lU4m/+h/d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AakashAhuja30/Pytorch/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTiAyqjDBOGN"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transform\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8jfS94xWrBg"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self, in_channels=1, num_classes=10):\n",
        "    super(CNN,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=1,out_channels=8,kernel_size=(3,3),stride=(1,1),padding=(1,1))\n",
        "    self.pool = nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))\n",
        "    self.conv2 = nn.Conv2d(in_channels=8,out_channels=16,kernel_size=(3,3),stride=(1,1),padding=(1,1))\n",
        "    self.fc1= nn.Linear(16*7*7, num_classes)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "    x = x.reshape(x.shape[0],-1)\n",
        "    x = self.fc1(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sENo6ddwKvSh"
      },
      "source": [
        "def save_checkpoint(state, filename = 'mycheckpoint.pth.tar'):\n",
        "  print(\"Saving Checkpoint\")\n",
        "  torch.save(state,filename)\n",
        "\n",
        "def load_checkpoint(checkpoint):\n",
        "  print('Loading Checkpoint')\n",
        "  model.load_state_dict(checkpoint['state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvOf6fEWXJ4D",
        "outputId": "0247ca6d-b4f5-4e88-a31a-593c8c6e7a9b"
      },
      "source": [
        "#Set Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJs3djCpYxko"
      },
      "source": [
        "#Hyperparameters\n",
        "input_channels = 1\n",
        "num_classes = 10\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "num_epochs = 10\n",
        "load_model = True"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpJDQMuTZPZA"
      },
      "source": [
        "#Load Data\n",
        "train_dataset = datasets.MNIST(root='dataset/',train = True, transform=transform.ToTensor(), download=True)\n",
        "train_loader = DataLoader(dataset = train_dataset , batch_size=batch_size,shuffle=True)\n",
        "test_dataset = datasets.MNIST(root='dataset/',train = False, transform=transform.ToTensor(), download=True)\n",
        "test_loader = DataLoader(dataset = test_dataset , batch_size=batch_size,shuffle=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILQmyv5DZSIq"
      },
      "source": [
        "model= CNN().to(device)\n",
        "#Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr=learning_rate)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDVDH2FiZz-I"
      },
      "source": [
        "def check_accuracy(loader,model):\n",
        "  if loader.dataset.train:\n",
        "    print('Checking accuracy on Train data')\n",
        "  else:\n",
        "    print('Checking accuracy on Test data')\n",
        "\n",
        "  num_samples =  0\n",
        "  num_correct = 0\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x,y in loader:\n",
        "      x = x.to(device = device)\n",
        "      y = y.to(device = device)\n",
        "\n",
        "      scores= model(x)\n",
        "      _,pred=torch.max(scores,1)\n",
        "      num_correct += (pred ==y).sum()\n",
        "      num_samples += pred.size(0)\n",
        "\n",
        "    print(f'Accuracy is :{(num_correct/num_samples)*100}')\n",
        "\n",
        "  model.train()  "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0wSPokdaUXD",
        "outputId": "31062a25-a060-46d0-9e8e-8196fa3ae525"
      },
      "source": [
        "if load_model:\n",
        "  load_checkpoint(torch.load('mycheckpoint.pth.tar'))\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  losses = []\n",
        "  checkpoint = {'state_dict': model.state_dict(),'optimizer':optimizer.state_dict()}\n",
        "\n",
        "  if epoch %3 ==0:\n",
        "    save_checkpoint(checkpoint)\n",
        "\n",
        "\n",
        "  for batch_idx, (data,target) in enumerate(train_loader):\n",
        "    data = data.to(device = device)\n",
        "    target = target.to(device = device)\n",
        "    \n",
        "    #Forward pass\n",
        "    output = model(data)\n",
        "    loss = criterion(output,target)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    #Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    #Gradient descent\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  loss_mean = float(np.mean(losses))\n",
        "  print(f'epoch{epoch+1} , Loss: {loss_mean}')\n",
        "  check_accuracy(train_loader,model)\n",
        "  check_accuracy(test_loader,model)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Checkpoint\n",
            "Saving Checkpoint\n",
            "epoch1 , Loss: 0.0383143088793251\n",
            "Checking accuracy on Train data\n",
            "Accuracy is :98.78333282470703\n",
            "Checking accuracy on Test data\n",
            "Accuracy is :98.43999481201172\n",
            "epoch2 , Loss: 0.034775494160301894\n",
            "Checking accuracy on Train data\n",
            "Accuracy is :99.07167053222656\n",
            "Checking accuracy on Test data\n",
            "Accuracy is :98.72999572753906\n",
            "epoch3 , Loss: 0.032180779007958994\n",
            "Checking accuracy on Train data\n",
            "Accuracy is :99.20833587646484\n",
            "Checking accuracy on Test data\n",
            "Accuracy is :98.64999389648438\n",
            "Saving Checkpoint\n",
            "epoch4 , Loss: 0.028942582640312374\n",
            "Checking accuracy on Train data\n",
            "Accuracy is :99.03333282470703\n",
            "Checking accuracy on Test data\n",
            "Accuracy is :98.5199966430664\n",
            "epoch5 , Loss: 0.0272563511928256\n",
            "Checking accuracy on Train data\n",
            "Accuracy is :99.23999786376953\n",
            "Checking accuracy on Test data\n",
            "Accuracy is :98.64999389648438\n",
            "epoch6 , Loss: 0.025852051654099317\n",
            "Checking accuracy on Train data\n",
            "Accuracy is :99.41500091552734\n",
            "Checking accuracy on Test data\n",
            "Accuracy is :98.77999877929688\n",
            "Saving Checkpoint\n",
            "epoch7 , Loss: 0.023784320806429886\n",
            "Checking accuracy on Train data\n",
            "Accuracy is :99.43167114257812\n",
            "Checking accuracy on Test data\n",
            "Accuracy is :98.77999877929688\n",
            "epoch8 , Loss: 0.023085837482755483\n",
            "Checking accuracy on Train data\n",
            "Accuracy is :99.50333404541016\n",
            "Checking accuracy on Test data\n",
            "Accuracy is :98.64999389648438\n",
            "epoch9 , Loss: 0.020191562687385523\n",
            "Checking accuracy on Train data\n",
            "Accuracy is :99.48666381835938\n",
            "Checking accuracy on Test data\n",
            "Accuracy is :98.67999267578125\n",
            "Saving Checkpoint\n",
            "epoch10 , Loss: 0.019526191218607207\n",
            "Checking accuracy on Train data\n",
            "Accuracy is :99.45999908447266\n",
            "Checking accuracy on Test data\n",
            "Accuracy is :98.7699966430664\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSvDcAQNMbRB"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}